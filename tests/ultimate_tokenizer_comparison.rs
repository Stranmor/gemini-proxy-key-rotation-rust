// tests/ultimate_tokenizer_comparison.rs

use std::env;
use std::error::Error;
use serde_json::{json, Value};
use reqwest::Client;
use tokio::time::{sleep, Duration};
use tracing::warn;
use gemini_proxy::tokenizer;

/// –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
#[tokio::test]
async fn test_ultimate_tokenizer_comparison() {
    tracing_subscriber::fmt::init();
    
    let api_key = match env::var("GOOGLE_API_KEY") {
        Ok(key) => key,
        Err(_) => {
            println!("GOOGLE_API_KEY not found, skipping ultimate comparison");
            return;
        }
    };
    
    println!("\nüöÄ ULTIMATE TOKENIZER COMPARISON üöÄ\n");
    
    // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã
    println!("üì¶ Initializing tokenizers...");
    
    // 1. –ü—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer::gemini_simple::GeminiTokenizer::initialize().await.unwrap();
    
    // 2. ML-–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–π
    tokenizer::gemini_ml_calibrated::GeminiMLCalibratedTokenizer::initialize().await.unwrap();
    
    // 3. –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π Google (–º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –µ—Å–ª–∏ Python SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
    let official_available = match tokenizer::official_google::OfficialGoogleTokenizer::initialize().await {
        Ok(_) => {
            println!("‚úÖ Official Google tokenizer available");
            true
        }
        Err(e) => {
            println!("‚ö†Ô∏è  Official Google tokenizer not available: {e}");
            println!("üí° Install with: pip install google-cloud-aiplatform[tokenization]");
            false
        }
    };
    
    // 4. –ü—Ä–æ–∫—Å–∏-–∫–µ—à–∏—Ä—É—é—â–∏–π
    let proxy_tokenizer = tokenizer::ProxyCachedTokenizer::new(api_key.clone())
        .with_fallback(|text| text.split_whitespace().count() + 2);
    
    // –¢–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏
    let test_cases = vec![
        ("Simple text", "Hello world"),
        ("Unicode heavy", "Hello ‰∏ñÁïå! üåç How are you? –ü—Ä–∏–≤–µ—Ç –º–∏—Ä!"),
        ("Math symbols", "Mathematical symbols: ‚àë, ‚à´, ‚àÇ, ‚àá, ‚àû, œÄ, Œ±, Œ≤"),
        ("Code snippet", r#"function fibonacci(n) { if (n <= 1) return n; return fibonacci(n-1) + fibonacci(n-2); }"#),
        ("JSON data", r#"{"name": "John", "age": 30, "city": "New York", "hobbies": ["reading", "coding"]}"#),
    ];
    
    let client = Client::new();
    
    println!("\nüìä COMPARISON RESULTS\n");
    println!("{:<15} | {:>8} | {:>8} | {:>8} | {:>8} | {:>8} | {:>8}", 
        "Test Case", "Google", "Simple", "ML-Cal", "Official", "Proxy", "Accuracy");
    println!("{:-<15}-+-{:->8}-+-{:->8}-+-{:->8}-+-{:->8}-+-{:->8}-+-{:->8}", 
        "", "", "", "", "", "", "");
    
    let mut total_tests = 0;
    let mut simple_accurate = 0;
    let mut ml_accurate = 0;
    let mut official_accurate = 0;
    let mut proxy_accurate = 0;
    
    for (name, text) in test_cases {
        // Google API (—ç—Ç–∞–ª–æ–Ω)
        let google_count = match get_google_token_count(&client, &api_key, text).await {
            Ok(count) => count,
            Err(e) => {
                warn!("Google API failed for {}: {}", name, e);
                sleep(Duration::from_millis(1000)).await;
                continue;
            }
        };
        
        // –ù–∞—à–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã
        let simple_count = tokenizer::count_gemini_tokens(text).unwrap_or(0);
        let ml_count = tokenizer::count_ml_calibrated_gemini_tokens(text).unwrap_or(0);
        
        let official_count = if official_available {
            tokenizer::count_official_google_tokens(text).unwrap_or(0)
        } else {
            0
        };
        
        let proxy_count = proxy_tokenizer.count_tokens(text).await.unwrap_or(0);
        
        // –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
        let simple_accuracy = calculate_accuracy(simple_count, google_count);
        let ml_accuracy = calculate_accuracy(ml_count, google_count);
        let official_accuracy = if official_available {
            calculate_accuracy(official_count, google_count)
        } else {
            0.0
        };
        let proxy_accuracy = calculate_accuracy(proxy_count, google_count);
        
        // –°—á–∏—Ç–∞–µ–º —Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (>95% —Ç–æ—á–Ω–æ—Å—Ç–∏)
        total_tests += 1;
        if simple_accuracy >= 95.0 { simple_accurate += 1; }
        if ml_accuracy >= 95.0 { ml_accurate += 1; }
        if official_accuracy >= 95.0 { official_accurate += 1; }
        if proxy_accuracy >= 95.0 { proxy_accurate += 1; }
        
        // –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        let official_str = if official_available {
            format!("{official_count:>8}")
        } else {
            "   N/A  ".to_string()
        };
        
        let best_accuracy = [simple_accuracy, ml_accuracy, official_accuracy, proxy_accuracy]
            .iter().fold(0.0f64, |a, &b| a.max(b));
        
        println!("{name:<15} | {google_count:>8} | {simple_count:>8} | {ml_count:>8} | {official_str} | {proxy_count:>8} | {best_accuracy:>7.1}%");
        
        sleep(Duration::from_millis(500)).await;
    }
    
    // –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    println!("\nüéØ FINAL RESULTS\n");
    
    let simple_score = (simple_accurate as f64 / total_tests as f64) * 100.0;
    let ml_score = (ml_accurate as f64 / total_tests as f64) * 100.0;
    let official_score = if official_available {
        (official_accurate as f64 / total_tests as f64) * 100.0
    } else {
        0.0
    };
    let proxy_score = (proxy_accurate as f64 / total_tests as f64) * 100.0;
    
    println!("üìà Accuracy Scores (>95% threshold):");
    println!("  Simple Tokenizer:     {simple_score:.1}%");
    println!("  ML-Calibrated:       {ml_score:.1}%");
    if official_available {
        println!("  Official Google:      {official_score:.1}% ‚≠ê");
    } else {
        println!("  Official Google:      Not Available");
    }
    println!("  Proxy-Cached:         {proxy_score:.1}%");
    
    // –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    println!("\nüèÜ RECOMMENDATIONS:\n");
    
    if official_available && official_score >= 95.0 {
        println!("ü•á WINNER: Official Google Tokenizer");
        println!("   ‚úÖ 100% accuracy guaranteed");
        println!("   ‚úÖ Always up-to-date with Google");
        println!("   ‚úÖ Supports all Gemini models");
        println!("   ‚ö†Ô∏è  Requires Python SDK");
        println!("   üí° Best for: Production systems requiring perfect accuracy");
    } else if proxy_score >= 95.0 {
        println!("ü•á WINNER: Proxy-Cached Tokenizer");
        println!("   ‚úÖ 100% accuracy (uses real Google API)");
        println!("   ‚úÖ Caching for performance");
        println!("   ‚úÖ Fallback support");
        println!("   ‚ö†Ô∏è  Requires API calls for new texts");
        println!("   üí° Best for: High-accuracy with caching");
    } else if ml_score >= 80.0 {
        println!("ü•à RUNNER-UP: ML-Calibrated Tokenizer");
        println!("   ‚úÖ Good accuracy ({ml_score:.1}%)");
        println!("   ‚úÖ No external dependencies");
        println!("   ‚úÖ Fast performance");
        println!("   üí° Best for: Offline systems with good accuracy");
    } else {
        println!("‚ö†Ô∏è  All tokenizers need improvement for your use case");
        println!("   üí° Consider using Proxy-Cached for 100% accuracy");
    }
    
    println!("\nüéØ CONCLUSION:");
    if official_available {
        println!("For 100% accuracy, use Official Google Tokenizer!");
        println!("Install: pip install google-cloud-aiplatform[tokenization]");
    } else {
        println!("For 100% accuracy, use Proxy-Cached Tokenizer!");
        println!("It uses real Google API with intelligent caching.");
    }
}

/// –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç Google API
async fn get_google_token_count(
    client: &Client, 
    api_key: &str, 
    text: &str
) -> Result<usize, Box<dyn Error + Send + Sync>> {
    let url = format!(
        "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:countTokens?key={api_key}"
    );
    
    let request_body = json!({
        "contents": [{
            "parts": [{
                "text": text
            }]
        }]
    });
    
    let response = client
        .post(&url)
        .header("Content-Type", "application/json")
        .json(&request_body)
        .timeout(Duration::from_secs(10))
        .send()
        .await?;
    
    if !response.status().is_success() {
        return Err(format!("Google API error: {}", response.status()).into());
    }
    
    let response_json: Value = response.json().await?;
    
    let total_tokens = response_json
        .get("totalTokens")
        .and_then(|t| t.as_u64())
        .ok_or("Missing totalTokens in response")?;
    
    Ok(total_tokens as usize)
}

/// –í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
fn calculate_accuracy(our_count: usize, google_count: usize) -> f64 {
    if google_count == 0 {
        return if our_count == 0 { 100.0 } else { 0.0 };
    }
    
    let diff = (our_count as i32 - google_count as i32).abs() as f64;
    let accuracy = (1.0 - (diff / google_count as f64)) * 100.0;
    accuracy.max(0.0)
}

/// –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
#[tokio::test]
async fn test_tokenizer_performance_comparison() {
    println!("\n‚ö° PERFORMANCE COMPARISON\n");
    
    // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã
    tokenizer::gemini_simple::GeminiTokenizer::initialize().await.unwrap();
    tokenizer::gemini_ml_calibrated::GeminiMLCalibratedTokenizer::initialize().await.unwrap();
    
    let official_available = tokenizer::official_google::OfficialGoogleTokenizer::initialize().await.is_ok();
    
    let test_text = "This is a comprehensive performance test for all tokenizer implementations with various content types including Unicode ‰∏ñÁïå üåç and code snippets.";
    let iterations = 100;
    
    // –¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    let start = std::time::Instant::now();
    for _ in 0..iterations {
        let _ = tokenizer::count_gemini_tokens(test_text).unwrap();
    }
    let simple_duration = start.elapsed();
    
    // –¢–µ—Å—Ç ML-–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ
    let start = std::time::Instant::now();
    for _ in 0..iterations {
        let _ = tokenizer::count_ml_calibrated_gemini_tokens(test_text).unwrap();
    }
    let ml_duration = start.elapsed();
    
    // –¢–µ—Å—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    let official_duration = if official_available {
        let start = std::time::Instant::now();
        for _ in 0..iterations {
            let _ = tokenizer::count_official_google_tokens(test_text).unwrap_or(0);
        }
        Some(start.elapsed())
    } else {
        None
    };
    
    println!("üèÉ Performance Results ({iterations} iterations):");
    println!("  Simple:        {:>8.2}ms avg ({:>6.2}ms total)", 
        simple_duration.as_millis() as f64 / iterations as f64,
        simple_duration.as_millis());
    println!("  ML-Calibrated: {:>8.2}ms avg ({:>6.2}ms total)", 
        ml_duration.as_millis() as f64 / iterations as f64,
        ml_duration.as_millis());
    
    if let Some(duration) = official_duration {
        println!("  Official:      {:>8.2}ms avg ({:>6.2}ms total)", 
            duration.as_millis() as f64 / iterations as f64,
            duration.as_millis());
    } else {
        println!("  Official:      Not Available");
    }
    
    println!("\nüí° Performance vs Accuracy Trade-off:");
    println!("  ‚Ä¢ Simple: Fastest, but lower accuracy");
    println!("  ‚Ä¢ ML-Calibrated: Good balance of speed and accuracy");
    println!("  ‚Ä¢ Official: Slower, but 100% accurate");
    println!("  ‚Ä¢ Proxy-Cached: Variable (fast for cached, slow for new)");
}