// tests/large_text_tokenizer_test.rs

use std::env;
use std::error::Error;
use serde_json::{json, Value};
use reqwest::Client;
use tokio::time::{sleep, Duration};
use tracing::warn;
use gemini_proxy::tokenizer;

/// Ğ¢ĞµÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…
#[tokio::test]
async fn test_large_text_tokenization() {
    tracing_subscriber::fmt::init();
    
    let api_key = match env::var("GOOGLE_API_KEY") {
        Ok(key) => key,
        Err(_) => {
            println!("GOOGLE_API_KEY not found, skipping large text test");
            return;
        }
    };
    
    println!("\nğŸ“š LARGE TEXT TOKENIZATION TEST\n");
    
    // Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹
    tokenizer::gemini_simple::GeminiTokenizer::initialize().await.unwrap();
    tokenizer::gemini_ml_calibrated::GeminiMLCalibratedTokenizer::initialize().await.unwrap();
    
    let proxy_tokenizer = tokenizer::ProxyCachedTokenizer::new(api_key.clone())
        .with_fallback(|text| text.split_whitespace().count() + text.len() / 20);
    
    // Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ²
    let large_texts = vec![
        (
            "Technical Documentation",
            generate_technical_doc(),
        ),
        (
            "Code File",
            generate_large_code_file(),
        ),
        (
            "Natural Language",
            generate_natural_language_text(),
        ),
        (
            "Mixed Content",
            generate_mixed_content(),
        ),
        (
            "Unicode Heavy",
            generate_unicode_heavy_text(),
        ),
    ];
    
    let client = Client::new();
    
    println!("{:<20} | {:>8} | {:>8} | {:>8} | {:>8} | {:>8} | {:>8}", 
        "Text Type", "Length", "Google", "Simple", "ML-Cal", "Proxy", "Accuracy");
    println!("{:-<20}-+-{:->8}-+-{:->8}-+-{:->8}-+-{:->8}-+-{:->8}-+-{:->8}", 
        "", "", "", "", "", "", "");
    
    let mut total_tests = 0;
    let mut proxy_perfect = 0;
    let mut simple_good = 0;
    let mut ml_good = 0;
    
    for (name, text) in large_texts {
        let text_length = text.len();
        
        println!("ğŸ” Testing: {} ({} chars)", name, text_length);
        
        // Google API (ÑÑ‚Ğ°Ğ»Ğ¾Ğ½)
        let google_count = match get_google_token_count(&client, &api_key, &text).await {
            Ok(count) => count,
            Err(e) => {
                warn!("Google API failed for {}: {}", name, e);
                sleep(Duration::from_millis(2000)).await;
                continue;
            }
        };
        
        // ĞĞ°ÑˆĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹
        let simple_count = tokenizer::count_gemini_tokens(&text).unwrap_or(0);
        let ml_count = tokenizer::count_ml_calibrated_gemini_tokens(&text).unwrap_or(0);
        let proxy_count = proxy_tokenizer.count_tokens(&text).await.unwrap_or(0);
        
        // Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ
        let simple_accuracy = calculate_accuracy(simple_count, google_count);
        let ml_accuracy = calculate_accuracy(ml_count, google_count);
        let proxy_accuracy = calculate_accuracy(proxy_count, google_count);
        
        total_tests += 1;
        if proxy_accuracy >= 99.0 { proxy_perfect += 1; }
        if simple_accuracy >= 85.0 { simple_good += 1; }
        if ml_accuracy >= 85.0 { ml_good += 1; }
        
        let best_accuracy = [simple_accuracy, ml_accuracy, proxy_accuracy]
            .iter().fold(0.0f64, |a, &b| a.max(b));
        
        println!("{:<20} | {:>8} | {:>8} | {:>8} | {:>8} | {:>8} | {:>7.1}%", 
            name, text_length, google_count, simple_count, ml_count, proxy_count, best_accuracy);
        
        // Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹
        if best_accuracy < 90.0 {
            println!("  âš ï¸  Large discrepancy detected!");
            println!("    Simple error: {}", (simple_count as i32 - google_count as i32).abs());
            println!("    ML error: {}", (ml_count as i32 - google_count as i32).abs());
            println!("    Proxy error: {}", (proxy_count as i32 - google_count as i32).abs());
        }
        
        sleep(Duration::from_millis(1000)).await;
    }
    
    // Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    println!("\nğŸ“Š LARGE TEXT RESULTS\n");
    
    let proxy_score = (proxy_perfect as f64 / total_tests as f64) * 100.0;
    let simple_score = (simple_good as f64 / total_tests as f64) * 100.0;
    let ml_score = (ml_good as f64 / total_tests as f64) * 100.0;
    
    println!("ğŸ¯ Performance on Large Texts:");
    println!("  Proxy-Cached (>99%):  {:.1}%", proxy_score);
    println!("  Simple (>85%):        {:.1}%", simple_score);
    println!("  ML-Calibrated (>85%): {:.1}%", ml_score);
    
    // Ğ¢ĞµÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…
    println!("\nâš¡ PERFORMANCE ON LARGE TEXTS\n");
    
    let large_text = generate_very_large_text();
    println!("Testing performance on {} character text", large_text.len());
    
    // ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€
    let start = std::time::Instant::now();
    let _ = tokenizer::count_gemini_tokens(&large_text).unwrap();
    let simple_time = start.elapsed();
    
    // ML-ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹
    let start = std::time::Instant::now();
    let _ = tokenizer::count_ml_calibrated_gemini_tokens(&large_text).unwrap();
    let ml_time = start.elapsed();
    
    println!("Performance Results:");
    println!("  Simple:        {:>8.2}ms", simple_time.as_millis());
    println!("  ML-Calibrated: {:>8.2}ms", ml_time.as_millis());
    
    // ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ğ°Ñ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²
    assert!(simple_time.as_millis() < 100, "Simple tokenizer too slow on large text");
    assert!(ml_time.as_millis() < 200, "ML tokenizer too slow on large text");
    
    println!("\nâœ… Large text tokenization test completed!");
}

/// Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ
fn generate_technical_doc() -> String {
    r#"
# Gemini API Token Counting Documentation

## Overview

The Gemini API uses tokens as the fundamental unit for processing text input and generating responses. Understanding token counting is crucial for:

1. **Cost Estimation**: Billing is based on token consumption
2. **Rate Limiting**: API limits are enforced per token
3. **Context Management**: Models have token-based context windows
4. **Performance Optimization**: Token count affects processing time

## Token Calculation Methods

### Method 1: Local Tokenization

```python
from vertexai.preview import tokenization

model_name = "gemini-1.5-flash-001"
tokenizer = tokenization.get_tokenizer_for_model(model_name)
result = tokenizer.count_tokens("Hello World!")
print(f"Tokens: {result.total_tokens}")
```

### Method 2: API-based Counting

```bash
curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:countTokens?key=$API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [{"text": "Hello World!"}]
    }]
  }'
```

## Best Practices

1. **Cache Results**: Token counts for identical text remain constant
2. **Batch Processing**: Group multiple texts for efficiency
3. **Monitor Usage**: Track token consumption for cost control
4. **Optimize Prompts**: Reduce unnecessary tokens in system prompts

## Model-Specific Considerations

- **Gemini 1.0 Pro**: 32,760 input tokens maximum
- **Gemini 1.5 Pro**: 2,097,152 input tokens maximum
- **Gemini 2.0 Flash**: 1,048,576 input tokens maximum

## Error Handling

Always implement proper error handling for token counting operations:

```rust
match tokenizer.count_tokens(text) {
    Ok(count) => println!("Token count: {}", count),
    Err(e) => eprintln!("Tokenization failed: {}", e),
}
```

## Performance Metrics

Typical tokenization performance:
- Small texts (<1KB): <1ms
- Medium texts (1-10KB): 1-5ms  
- Large texts (10-100KB): 5-50ms
- Very large texts (>100KB): 50-500ms

## Troubleshooting

Common issues and solutions:

1. **Inconsistent Counts**: Ensure using same model version
2. **Performance Issues**: Consider caching for repeated texts
3. **API Errors**: Implement retry logic with exponential backoff
4. **Memory Usage**: Monitor cache size for long-running applications
"#.to_string()
}

/// Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ„Ğ°Ğ¹Ğ» ĞºĞ¾Ğ´Ğ°
fn generate_large_code_file() -> String {
    r#"
// Large Rust code file for tokenization testing
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use tokio::time::{sleep, Duration};
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenizerConfig {
    pub model_name: String,
    pub max_tokens: usize,
    pub temperature: f64,
    pub top_p: f64,
    pub cache_enabled: bool,
    pub cache_size_mb: usize,
    pub timeout_secs: u64,
}

impl Default for TokenizerConfig {
    fn default() -> Self {
        Self {
            model_name: "gemini-2.0-flash".to_string(),
            max_tokens: 1048576,
            temperature: 0.7,
            top_p: 0.9,
            cache_enabled: true,
            cache_size_mb: 100,
            timeout_secs: 30,
        }
    }
}

pub struct AdvancedTokenizer {
    config: TokenizerConfig,
    cache: Arc<Mutex<HashMap<String, usize>>>,
    stats: Arc<Mutex<TokenizerStats>>,
}

#[derive(Debug, Default)]
struct TokenizerStats {
    total_requests: u64,
    cache_hits: u64,
    cache_misses: u64,
    total_tokens_processed: u64,
    average_processing_time_ms: f64,
    errors: u64,
}

impl AdvancedTokenizer {
    pub fn new(config: TokenizerConfig) -> Self {
        Self {
            config,
            cache: Arc::new(Mutex::new(HashMap::new())),
            stats: Arc::new(Mutex::new(TokenizerStats::default())),
        }
    }
    
    pub async fn count_tokens(&self, text: &str) -> Result<usize, TokenizerError> {
        let start_time = std::time::Instant::now();
        
        // Update stats
        {
            let mut stats = self.stats.lock().unwrap();
            stats.total_requests += 1;
        }
        
        // Check cache first
        if self.config.cache_enabled {
            let cache_key = self.generate_cache_key(text);
            
            if let Ok(cache) = self.cache.lock() {
                if let Some(&cached_count) = cache.get(&cache_key) {
                    let mut stats = self.stats.lock().unwrap();
                    stats.cache_hits += 1;
                    return Ok(cached_count);
                }
            }
        }
        
        // Perform actual tokenization
        let token_count = self.perform_tokenization(text).await?;
        
        // Update cache
        if self.config.cache_enabled {
            if let Ok(mut cache) = self.cache.lock() {
                let cache_key = self.generate_cache_key(text);
                cache.insert(cache_key, token_count);
                
                // Cleanup cache if too large
                if cache.len() > 10000 {
                    cache.clear();
                }
            }
        }
        
        // Update stats
        {
            let mut stats = self.stats.lock().unwrap();
            stats.cache_misses += 1;
            stats.total_tokens_processed += token_count as u64;
            
            let processing_time = start_time.elapsed().as_millis() as f64;
            stats.average_processing_time_ms = 
                (stats.average_processing_time_ms * (stats.total_requests - 1) as f64 + processing_time) 
                / stats.total_requests as f64;
        }
        
        Ok(token_count)
    }
    
    async fn perform_tokenization(&self, text: &str) -> Result<usize, TokenizerError> {
        // Simulate different tokenization strategies
        match self.config.model_name.as_str() {
            "gemini-1.0-pro" => self.tokenize_v1(text).await,
            "gemini-1.5-pro" | "gemini-1.5-flash" => self.tokenize_v15(text).await,
            "gemini-2.0-flash" => self.tokenize_v2(text).await,
            _ => Err(TokenizerError::UnsupportedModel(self.config.model_name.clone())),
        }
    }
    
    async fn tokenize_v1(&self, text: &str) -> Result<usize, TokenizerError> {
        // Gemini 1.0 tokenization logic
        let base_count = text.split_whitespace().count();
        let punctuation_count = text.chars().filter(|c| c.is_ascii_punctuation()).count();
        Ok(base_count + punctuation_count / 2)
    }
    
    async fn tokenize_v15(&self, text: &str) -> Result<usize, TokenizerError> {
        // Gemini 1.5 tokenization logic (more sophisticated)
        let words = text.split_whitespace().collect::<Vec<_>>();
        let mut token_count = 0;
        
        for word in words {
            token_count += match word.len() {
                0 => 0,
                1..=4 => 1,
                5..=8 => if word.contains('-') || word.contains('_') { 2 } else { 1 },
                9..=12 => 2,
                _ => (word.len() + 3) / 4,
            };
        }
        
        // Add tokens for punctuation and special characters
        let special_chars = text.chars().filter(|c| !c.is_alphanumeric() && !c.is_whitespace()).count();
        token_count += special_chars / 2;
        
        Ok(token_count)
    }
    
    async fn tokenize_v2(&self, text: &str) -> Result<usize, TokenizerError> {
        // Gemini 2.0 tokenization logic (most advanced)
        let mut token_count = 0;
        let chars: Vec<char> = text.chars().collect();
        let mut i = 0;
        
        while i < chars.len() {
            if chars[i].is_whitespace() {
                i += 1;
                continue;
            }
            
            // Extract word or token
            let start = i;
            while i < chars.len() && !chars[i].is_whitespace() {
                i += 1;
            }
            
            let token = chars[start..i].iter().collect::<String>();
            token_count += self.estimate_token_count_v2(&token);
        }
        
        Ok(token_count)
    }
    
    fn estimate_token_count_v2(&self, token: &str) -> usize {
        // Advanced token estimation for Gemini 2.0
        let len = token.chars().count();
        let has_numbers = token.chars().any(|c| c.is_ascii_digit());
        let has_special = token.chars().any(|c| c.is_ascii_punctuation());
        let has_unicode = token.chars().any(|c| !c.is_ascii());
        
        let base_tokens = match len {
            0 => 0,
            1..=3 => 1,
            4..=6 => if has_special || has_numbers { 2 } else { 1 },
            7..=10 => if has_unicode { 2 } else { 1 + (len - 6) / 3 },
            _ => (len + 2) / 3,
        };
        
        base_tokens.max(1)
    }
    
    fn generate_cache_key(&self, text: &str) -> String {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        text.hash(&mut hasher);
        self.config.model_name.hash(&mut hasher);
        format!("{:x}", hasher.finish())
    }
    
    pub fn get_stats(&self) -> TokenizerStats {
        self.stats.lock().unwrap().clone()
    }
    
    pub fn clear_cache(&self) {
        if let Ok(mut cache) = self.cache.lock() {
            cache.clear();
        }
    }
}

#[derive(Debug, thiserror::Error)]
pub enum TokenizerError {
    #[error("Unsupported model: {0}")]
    UnsupportedModel(String),
    
    #[error("Tokenization failed: {0}")]
    TokenizationFailed(String),
    
    #[error("Cache error: {0}")]
    CacheError(String),
    
    #[error("Configuration error: {0}")]
    ConfigError(String),
}

// Helper functions and utilities
pub mod utils {
    use super::*;
    
    pub fn estimate_tokens_simple(text: &str) -> usize {
        // Simple estimation: ~4 characters per token
        (text.len() + 3) / 4
    }
    
    pub fn estimate_tokens_by_words(text: &str) -> usize {
        // Word-based estimation
        let words = text.split_whitespace().count();
        let punctuation = text.chars().filter(|c| c.is_ascii_punctuation()).count();
        words + punctuation / 2
    }
    
    pub fn analyze_text_complexity(text: &str) -> TextComplexity {
        let total_chars = text.len();
        let words = text.split_whitespace().count();
        let sentences = text.matches('.').count() + text.matches('!').count() + text.matches('?').count();
        let unicode_chars = text.chars().filter(|c| !c.is_ascii()).count();
        let numbers = text.chars().filter(|c| c.is_ascii_digit()).count();
        let punctuation = text.chars().filter(|c| c.is_ascii_punctuation()).count();
        
        TextComplexity {
            total_chars,
            words,
            sentences,
            unicode_chars,
            numbers,
            punctuation,
            avg_word_length: if words > 0 { total_chars as f64 / words as f64 } else { 0.0 },
            complexity_score: calculate_complexity_score(total_chars, words, unicode_chars, punctuation),
        }
    }
    
    fn calculate_complexity_score(chars: usize, words: usize, unicode: usize, punct: usize) -> f64 {
        let base_score = chars as f64 / 100.0;
        let word_factor = if words > 0 { chars as f64 / words as f64 } else { 1.0 };
        let unicode_factor = 1.0 + (unicode as f64 / chars as f64) * 0.5;
        let punct_factor = 1.0 + (punct as f64 / chars as f64) * 0.3;
        
        base_score * word_factor * unicode_factor * punct_factor
    }
}

#[derive(Debug, Clone)]
pub struct TextComplexity {
    pub total_chars: usize,
    pub words: usize,
    pub sentences: usize,
    pub unicode_chars: usize,
    pub numbers: usize,
    pub punctuation: usize,
    pub avg_word_length: f64,
    pub complexity_score: f64,
}

// Integration tests
#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_advanced_tokenizer() {
        let config = TokenizerConfig::default();
        let tokenizer = AdvancedTokenizer::new(config);
        
        let test_text = "Hello, world! This is a test of the advanced tokenizer.";
        let count = tokenizer.count_tokens(test_text).await.unwrap();
        
        assert!(count > 0);
        assert!(count < test_text.len());
        
        let stats = tokenizer.get_stats();
        assert_eq!(stats.total_requests, 1);
        assert_eq!(stats.cache_misses, 1);
    }
    
    #[test]
    fn test_text_complexity_analysis() {
        let text = "Hello, world! ä¸–ç•Œ ğŸŒ How are you today?";
        let complexity = utils::analyze_text_complexity(text);
        
        assert!(complexity.unicode_chars > 0);
        assert!(complexity.punctuation > 0);
        assert!(complexity.complexity_score > 0.0);
    }
}
"#.to_string()
}

/// Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº
fn generate_natural_language_text() -> String {
    r#"
The Evolution of Artificial Intelligence and Its Impact on Modern Society

Artificial Intelligence (AI) has emerged as one of the most transformative technologies of the 21st century, fundamentally reshaping how we interact with digital systems, process information, and solve complex problems. From its humble beginnings in the 1950s as a theoretical concept explored by pioneers like Alan Turing and John McCarthy, AI has evolved into a sophisticated field encompassing machine learning, deep learning, natural language processing, computer vision, and robotics.

The journey of AI development can be traced through several distinct phases, each marked by significant breakthroughs and paradigm shifts. The early symbolic AI era focused on rule-based systems and expert knowledge representation, leading to the development of expert systems that could mimic human decision-making in specific domains. However, these systems were limited by their inability to learn from experience and adapt to new situations.

The introduction of machine learning algorithms in the 1980s and 1990s marked a crucial turning point, enabling systems to improve their performance through exposure to data rather than explicit programming. This shift from rule-based to data-driven approaches opened new possibilities for pattern recognition, predictive modeling, and automated decision-making across various industries.

The deep learning revolution of the 2010s, powered by advances in neural network architectures and the availability of massive datasets, has accelerated AI capabilities exponentially. Convolutional neural networks have revolutionized computer vision, enabling applications ranging from medical image analysis to autonomous vehicle navigation. Recurrent neural networks and transformer architectures have transformed natural language processing, making possible sophisticated language models that can generate human-like text, translate between languages, and engage in meaningful conversations.

Today's AI systems demonstrate remarkable capabilities across diverse domains. In healthcare, AI assists in diagnostic imaging, drug discovery, and personalized treatment recommendations. Financial institutions leverage AI for fraud detection, algorithmic trading, and risk assessment. Manufacturing companies employ AI-powered robotics for quality control, predictive maintenance, and supply chain optimization. Entertainment platforms use recommendation algorithms to personalize content delivery, while search engines employ sophisticated ranking algorithms to provide relevant results to billions of users daily.

The integration of AI into everyday life has been particularly evident in the proliferation of virtual assistants, smart home devices, and mobile applications that understand natural language commands and adapt to user preferences. These consumer-facing applications have made AI more accessible and familiar to the general public, demonstrating the technology's potential to enhance productivity and convenience.

However, the rapid advancement of AI also raises important ethical, social, and economic considerations. Concerns about job displacement due to automation have sparked debates about the future of work and the need for reskilling programs. Issues related to algorithmic bias, privacy protection, and the concentration of AI capabilities in the hands of a few large corporations have prompted calls for regulatory frameworks and ethical guidelines.

The development of increasingly powerful AI systems has also reignited discussions about artificial general intelligence (AGI) and its potential implications for humanity. While current AI systems excel in narrow domains, the prospect of machines achieving human-level intelligence across all cognitive tasks remains a subject of intense research and speculation.

As we look toward the future, several trends are likely to shape the continued evolution of AI. Edge computing will enable more efficient and privacy-preserving AI applications by processing data locally rather than in centralized cloud servers. Federated learning approaches will allow AI models to be trained across distributed datasets without compromising data privacy. Explainable AI techniques will make machine learning models more interpretable and trustworthy, addressing concerns about black-box decision-making.

The convergence of AI with other emerging technologies such as quantum computing, biotechnology, and nanotechnology promises to unlock new frontiers of innovation. Quantum machine learning algorithms could solve optimization problems that are intractable for classical computers, while AI-driven drug discovery platforms could accelerate the development of life-saving medications.

Education and workforce development will play crucial roles in ensuring that society can adapt to and benefit from AI advancements. Universities and training institutions are already incorporating AI and data science curricula to prepare the next generation of professionals. Lifelong learning programs will become increasingly important as the pace of technological change continues to accelerate.

International cooperation and governance frameworks will be essential for addressing the global challenges and opportunities presented by AI. Collaborative efforts to establish ethical standards, safety protocols, and beneficial AI development practices will help ensure that the technology serves the common good while minimizing potential risks.

In conclusion, artificial intelligence represents both a remarkable achievement of human ingenuity and a powerful tool for addressing some of our most pressing challenges. As we continue to push the boundaries of what machines can accomplish, it is crucial that we remain mindful of the broader implications and work collectively to harness AI's potential for the benefit of all humanity. The future of AI is not predetermined but will be shaped by the choices we make today regarding research priorities, ethical considerations, and societal values.
"#.to_string()
}

/// Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚
fn generate_mixed_content() -> String {
    r#"
# API Integration Guide: Gemini Tokenization Service

## Overview æ¦‚è¦ ĞĞ±Ğ·Ğ¾Ñ€

This document provides comprehensive guidance for integrating with the Gemini tokenization service. The service supports multiple languages including English, ä¸­æ–‡, Ğ ÑƒÑÑĞºĞ¸Ğ¹, EspaÃ±ol, FranÃ§ais, Deutsch, æ—¥æœ¬èª, and Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

## Quick Start å¿«é€Ÿå¼€å§‹

```bash
# Install dependencies
npm install @google/generative-ai
pip install google-generativeai
cargo add tokio reqwest serde_json

# Set environment variables
export GOOGLE_API_KEY="your-api-key-here"
export TOKENIZER_ENDPOINT="https://api.gemini.google.com/v1/tokenize"
```

### JavaScript Example

```javascript
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);

async function countTokens(text) {
  try {
    const model = genAI.getGenerativeModel({ model: "gemini-pro" });
    const result = await model.countTokens(text);
    return result.totalTokens;
  } catch (error) {
    console.error("Tokenization failed:", error);
    throw error;
  }
}

// Usage examples
const examples = [
  "Hello, world! ğŸŒ",
  "Mathematical equation: E = mcÂ²",
  "Code snippet: const x = (a, b) => a + b;",
  "Unicode text: ä½ å¥½ä¸–ç•Œ Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù… Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹ Ğ¼Ğ¸Ñ€"
];

for (const text of examples) {
  const tokens = await countTokens(text);
  console.log(`"${text}" â†’ ${tokens} tokens`);
}
```

### Python Example

```python
import google.generativeai as genai
import os
from typing import List, Dict, Any

# Configure the API
genai.configure(api_key=os.environ['GOOGLE_API_KEY'])

class TokenCounter:
    def __init__(self, model_name: str = "gemini-pro"):
        self.model = genai.GenerativeModel(model_name)
    
    def count_tokens(self, text: str) -> int:
        """Count tokens in the given text."""
        try:
            response = self.model.count_tokens(text)
            return response.total_tokens
        except Exception as e:
            print(f"Error counting tokens: {e}")
            return 0
    
    def batch_count(self, texts: List[str]) -> Dict[str, int]:
        """Count tokens for multiple texts."""
        results = {}
        for i, text in enumerate(texts):
            try:
                count = self.count_tokens(text)
                results[f"text_{i}"] = count
            except Exception as e:
                print(f"Error processing text {i}: {e}")
                results[f"text_{i}"] = 0
        return results

# Usage
counter = TokenCounter()

test_cases = [
    "Simple English text",
    "Text with Ã©mojis ğŸ˜€ğŸ‰ğŸš€ and spÃ«cial characters",
    "Code: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)",
    "Mixed languages: Hello ä½ å¥½ ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ù…Ø±Ø­Ø¨Ø§ Bonjour Hola ã“ã‚“ã«ã¡ã¯",
    "JSON: {\"name\": \"John\", \"age\": 30, \"skills\": [\"Python\", \"JavaScript\", \"Rust\"]}",
    "Mathematical notation: âˆ‘(i=1 to n) iÂ² = n(n+1)(2n+1)/6"
]

for text in test_cases:
    tokens = counter.count_tokens(text)
    print(f"Tokens: {tokens:3d} | Text: {text[:50]}{'...' if len(text) > 50 else ''}")
```

### Rust Example

```rust
use reqwest::Client;
use serde_json::{json, Value};
use std::error::Error;
use tokio;

#[derive(Debug)]
struct TokenizerClient {
    client: Client,
    api_key: String,
    base_url: String,
}

impl TokenizerClient {
    fn new(api_key: String) -> Self {
        Self {
            client: Client::new(),
            api_key,
            base_url: "https://generativelanguage.googleapis.com/v1beta".to_string(),
        }
    }
    
    async fn count_tokens(&self, text: &str, model: &str) -> Result<usize, Box<dyn Error>> {
        let url = format!("{}/models/{}:countTokens?key={}", 
            self.base_url, model, self.api_key);
        
        let payload = json!({
            "contents": [{
                "parts": [{"text": text}]
            }]
        });
        
        let response = self.client
            .post(&url)
            .header("Content-Type", "application/json")
            .json(&payload)
            .send()
            .await?;
        
        let result: Value = response.json().await?;
        let token_count = result["totalTokens"]
            .as_u64()
            .ok_or("Invalid response format")?;
        
        Ok(token_count as usize)
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    let api_key = std::env::var("GOOGLE_API_KEY")
        .expect("GOOGLE_API_KEY environment variable not set");
    
    let client = TokenizerClient::new(api_key);
    
    let test_texts = vec![
        "Hello, Rust! ğŸ¦€",
        "Complex text with ä¸­æ–‡, Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©, and Ğ ÑƒÑÑĞºĞ¸Ğ¹",
        "fn main() { println!(\"Hello, world!\"); }",
        "Data: [1, 2, 3, 4, 5] â†’ sum = 15, avg = 3.0",
    ];
    
    for text in test_texts {
        match client.count_tokens(text, "gemini-pro").await {
            Ok(count) => println!("âœ… {} tokens: {}", count, text),
            Err(e) => println!("âŒ Error: {} for text: {}", e, text),
        }
    }
    
    Ok(())
}
```

## Performance Benchmarks æ€§èƒ½åŸºå‡†

| Text Type | Length | Tokens | Ratio | Processing Time |
|-----------|--------|--------|-------|-----------------|
| English | 1,000 chars | ~250 tokens | 4.0:1 | 15ms |
| Chinese ä¸­æ–‡ | 1,000 chars | ~500 tokens | 2.0:1 | 18ms |
| Code | 1,000 chars | ~300 tokens | 3.3:1 | 12ms |
| Mixed | 1,000 chars | ~280 tokens | 3.6:1 | 20ms |
| JSON | 1,000 chars | ~320 tokens | 3.1:1 | 14ms |

## Error Handling é”™è¯¯å¤„ç†

Common error scenarios and solutions:

### Rate Limiting (429)
```javascript
async function countTokensWithRetry(text, maxRetries = 3) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await countTokens(text);
    } catch (error) {
      if (error.status === 429 && i < maxRetries - 1) {
        await new Promise(resolve => setTimeout(resolve, Math.pow(2, i) * 1000));
        continue;
      }
      throw error;
    }
  }
}
```

### Invalid Input (400)
```python
def validate_input(text: str) -> bool:
    if not text or not isinstance(text, str):
        return False
    if len(text) > 1_000_000:  # 1MB limit
        return False
    return True

def safe_count_tokens(text: str) -> int:
    if not validate_input(text):
        raise ValueError("Invalid input text")
    return counter.count_tokens(text)
```

## Best Practices æœ€ä½³å®è·µ

1. **Caching**: Cache token counts for repeated texts
2. **Batching**: Process multiple texts in batches when possible
3. **Monitoring**: Track API usage and performance metrics
4. **Fallback**: Implement local estimation for offline scenarios

## Troubleshooting æ•…éšœæ’é™¤

### Common Issues:

- **Inconsistent counts**: Ensure same model version
- **Timeout errors**: Increase timeout for large texts
- **Memory issues**: Process large texts in chunks
- **Unicode problems**: Use proper encoding (UTF-8)

### Debug Mode:

```bash
export DEBUG_TOKENIZER=true
export LOG_LEVEL=debug
```

## Support æ”¯æŒ

For technical support:
- ğŸ“§ Email: support@example.com
- ğŸ’¬ Discord: https://discord.gg/example
- ğŸ“š Documentation: https://docs.example.com
- ğŸ› Issues: https://github.com/example/tokenizer/issues

---

Â© 2024 Tokenization Service. All rights reserved.
Licensed under MIT License.
"#.to_string()
}

/// Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Unicode
fn generate_unicode_heavy_text() -> String {
    r#"
ğŸŒ å¤šè¯­è¨€æ–‡æœ¬å¤„ç†ç³»ç»Ÿ ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª

## æ¦‚è¿° Overview ĞĞ±Ğ·Ğ¾Ñ€ Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

è¿™ä¸ªç³»ç»Ÿæ”¯æŒå¤šç§è¯­è¨€å’Œå­—ç¬¦é›†çš„å¤„ç†ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
This system supports processing of multiple languages and character sets, including but not limited to:
Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ, Ğ½Ğ¾ Ğ½Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑÑŒ:
ÙŠØ¯Ø¹Ù… Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„ØºØ§Øª ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø§Øª Ø£Ø­Ø±Ù Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ Ù„Ø§ Ø§Ù„Ø­ØµØ±:

### æ”¯æŒçš„è¯­è¨€ Supported Languages ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©

1. **ä¸­æ–‡ Chinese ä¸­å›½è¯**
   - ç®€ä½“ä¸­æ–‡ï¼šä½ å¥½ä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚
   - ç¹é«”ä¸­æ–‡ï¼šä½ å¥½ä¸–ç•Œï¼é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬ã€‚
   - å¤æ–‡ï¼šå­æ›°ï¼šã€Œå­¸è€Œæ™‚ç¿’ä¹‹ï¼Œä¸äº¦èªªä¹ï¼Ÿã€

2. **English**
   - Standard: Hello world! This is a test text.
   - Technical: API endpoints, JSON parsing, HTTP requests
   - Colloquial: Hey there! What's up? How's it going?

3. **Ğ ÑƒÑÑĞºĞ¸Ğ¹ Russian**
   - Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹: ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€! Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚.
   - Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹: API Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹, Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ JSON, HTTP Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹
   - Ğ Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ğ¹: ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! ĞšĞ°Ğº Ğ´ĞµĞ»Ğ°? Ğ§Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾?

4. **Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Arabic**
   - Ù…Ø¹ÙŠØ§Ø±ÙŠ: Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…! Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ.
   - ØªÙ‚Ù†ÙŠ: ÙˆØ§Ø¬Ù‡Ø§Øª Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§ØªØŒ ØªØ­Ù„ÙŠÙ„ JSONØŒ Ø·Ù„Ø¨Ø§Øª HTTP
   - Ø¹Ø§Ù…ÙŠ: Ø£Ù‡Ù„Ø§Ù‹! ÙƒÙŠÙ Ø§Ù„Ø­Ø§Ù„ØŸ Ø¥ÙŠØ´ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±ØŸ

5. **æ—¥æœ¬èª Japanese**
   - ã²ã‚‰ãŒãªï¼šã“ã‚“ã«ã¡ã¯ ã›ã‹ã„ï¼ã“ã‚Œã¯ ã¦ã™ã¨ ã® ã¶ã‚“ã—ã‚‡ã† ã§ã™ã€‚
   - ã‚«ã‚¿ã‚«ãƒŠï¼šã‚³ãƒ³ãƒ‹ãƒãƒ ã‚»ã‚«ã‚¤ï¼ã‚³ãƒ¬ãƒ ãƒ†ã‚¹ãƒˆ ãƒ ãƒ–ãƒ³ã‚·ãƒ§ã‚¦ ãƒ‡ã‚¹ã€‚
   - æ¼¢å­—ï¼šä»Šæ—¥ã¯ä¸–ç•Œï¼ã“ã‚Œã¯è©¦é¨“ã®æ–‡ç« ã§ã™ã€‚
   - æ··åˆï¼šHelloä¸–ç•Œï¼ã“ã‚Œã¯testã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚

6. **í•œêµ­ì–´ Korean**
   - í•œê¸€: ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„! ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
   - í•œì: å®‰å¯§í•˜ì„¸ìš” ä¸–ç•Œ! ì´ê²ƒì€ è©¦é©— í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

7. **EspaÃ±ol Spanish**
   - EstÃ¡ndar: Â¡Hola mundo! Este es un texto de prueba.
   - TÃ©cnico: APIs, anÃ¡lisis JSON, peticiones HTTP
   - Coloquial: Â¡Hola! Â¿QuÃ© tal? Â¿CÃ³mo va todo?

8. **FranÃ§ais French**
   - Standard: Bonjour le monde! Ceci est un texte de test.
   - Technique: APIs, analyse JSON, requÃªtes HTTP
   - Familier: Salut! Ã‡a va? Quoi de neuf?

9. **Deutsch German**
   - Standard: Hallo Welt! Dies ist ein Testtext.
   - Technisch: APIs, JSON-Parsing, HTTP-Anfragen
   - Umgangssprachlich: Hallo! Wie geht's? Was gibt's Neues?

### ç‰¹æ®Šå­—ç¬¦å’Œç¬¦å· Special Characters and Symbols Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©

#### æ•°å­¦ç¬¦å· Mathematical Symbols ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©
- åŸºæœ¬è¿ç®—ï¼š+ - Ã— Ã· = â‰  â‰ˆ â‰¤ â‰¥ Â± âˆ“
- é«˜çº§æ•°å­¦ï¼šâˆ‘ âˆ âˆ« âˆ® âˆ‚ âˆ‡ âˆ âˆ… âˆˆ âˆ‰ âŠ‚ âŠƒ âˆª âˆ©
- å¸Œè…Šå­—æ¯ï¼šÎ± Î² Î³ Î´ Îµ Î¶ Î· Î¸ Î¹ Îº Î» Î¼ Î½ Î¾ Î¿ Ï€ Ï Ïƒ Ï„ Ï… Ï† Ï‡ Ïˆ Ï‰
- ä¸Šæ ‡ä¸‹æ ‡ï¼šxÂ² yÂ³ Hâ‚‚O COâ‚‚ E=mcÂ² aâ‚ + aâ‚‚ = aâ‚ƒ

#### è´§å¸ç¬¦å· Currency Symbols Ğ’Ğ°Ğ»ÑÑ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ Ø±Ù…ÙˆØ² Ø§Ù„Ø¹Ù…Ù„Ø§Øª
- å¸¸ç”¨è´§å¸ï¼š$ â‚¬ Â£ Â¥ â‚¹ â‚½ â‚© â‚ª â‚¦ â‚¡ â‚¨ â‚« â‚± â‚µ â‚´ â‚¸ â‚¼ â‚¾
- åŠ å¯†è´§å¸ï¼šâ‚¿ Î Å Ã âŸ

#### è¡¨æƒ…ç¬¦å· Emojis Ğ­Ğ¼Ğ¾Ğ´Ğ·Ğ¸ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ©
- é¢éƒ¨è¡¨æƒ…ï¼šğŸ˜€ ğŸ˜ƒ ğŸ˜„ ğŸ˜ ğŸ˜† ğŸ˜… ğŸ˜‚ ğŸ¤£ ğŸ˜Š ğŸ˜‡ ğŸ™‚ ğŸ™ƒ ğŸ˜‰ ğŸ˜Œ ğŸ˜ ğŸ¥° ğŸ˜˜ ğŸ˜— ğŸ˜™ ğŸ˜š ğŸ˜‹ ğŸ˜› ğŸ˜ ğŸ˜œ ğŸ¤ª ğŸ¤¨ ğŸ§ ğŸ¤“ ğŸ˜ ğŸ¤© ğŸ¥³
- æ‰‹åŠ¿ï¼šğŸ‘ ğŸ‘ ğŸ‘Œ ğŸ¤Œ ğŸ¤ âœŒï¸ ğŸ¤ ğŸ¤Ÿ ğŸ¤˜ ğŸ¤™ ğŸ‘ˆ ğŸ‘‰ ğŸ‘† ğŸ–• ğŸ‘‡ â˜ï¸ ğŸ‘‹ ğŸ¤š ğŸ–ï¸ âœ‹ ğŸ–– ğŸ‘ ğŸ™Œ ğŸ¤² ğŸ¤ ğŸ™
- åŠ¨ç‰©ï¼šğŸ¶ ğŸ± ğŸ­ ğŸ¹ ğŸ° ğŸ¦Š ğŸ» ğŸ¼ ğŸ¨ ğŸ¯ ğŸ¦ ğŸ® ğŸ· ğŸ¸ ğŸµ ğŸ™ˆ ğŸ™‰ ğŸ™Š ğŸ’ ğŸ” ğŸ§ ğŸ¦ ğŸ¤ ğŸ£ ğŸ¥ ğŸ¦† ğŸ¦… ğŸ¦‰ ğŸ¦‡ ğŸº ğŸ—
- é£Ÿç‰©ï¼šğŸ ğŸŠ ğŸ‹ ğŸŒ ğŸ‰ ğŸ‡ ğŸ“ ğŸ« ğŸˆ ğŸ’ ğŸ‘ ğŸ¥­ ğŸ ğŸ¥¥ ğŸ¥ ğŸ… ğŸ† ğŸ¥‘ ğŸ¥¦ ğŸ¥¬ ğŸ¥’ ğŸŒ¶ï¸ ğŸ«‘ ğŸŒ½ ğŸ¥• ğŸ«’ ğŸ§„ ğŸ§… ğŸ¥” ğŸ  ğŸ¥ ğŸ¥¯ ğŸ ğŸ¥– ğŸ¥¨ ğŸ§€ ğŸ¥š ğŸ³ ğŸ§ˆ ğŸ¥ ğŸ§‡ ğŸ¥“ ğŸ¥© ğŸ— ğŸ– ğŸ¦´ ğŸŒ­ ğŸ” ğŸŸ ğŸ•

#### æŠ€æœ¯ç¬¦å· Technical Symbols Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªÙ‚Ù†ÙŠØ©
- ç¼–ç¨‹ï¼š{ } [ ] ( ) < > / \ | & ^ ~ ` @ # % * + - = _ : ; " ' ? ! . ,
- ç½‘ç»œï¼š@ # $ % ^ & * ( ) - _ + = { } [ ] | \ : ; " ' < > , . ? / ~
- ç®­å¤´ï¼šâ† â†’ â†‘ â†“ â†” â†• â†– â†— â†˜ â†™ â‡ â‡’ â‡‘ â‡“ â‡” â‡• â‡– â‡— â‡˜ â‡™ â¡ï¸ â¬…ï¸ â¬†ï¸ â¬‡ï¸

### æµ‹è¯•ç”¨ä¾‹ Test Cases Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±

1. **æ··åˆè¯­è¨€æ–‡æœ¬ Mixed Language Text**
   Helloä¸–ç•Œ! ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€! Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…! Bonjour le monde! Â¡Hola mundo! ã“ã‚“ã«ã¡ã¯ä¸–ç•Œï¼ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„ï¼

2. **æŠ€æœ¯æ–‡æ¡£ Technical Documentation**
   ```javascript
   const API_URL = 'https://api.example.com/v1/tokenize';
   const response = await fetch(API_URL, {
     method: 'POST',
     headers: { 'Content-Type': 'application/json' },
     body: JSON.stringify({ text: 'ä½ å¥½ä¸–ç•Œï¼Hello world! Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…!' })
   });
   ```

3. **æ•°å­¦å…¬å¼ Mathematical Formulas**
   E = mcÂ² | F = ma | aÂ² + bÂ² = cÂ² | âˆ«â‚€^âˆ e^(-xÂ²) dx = âˆšÏ€/2 | âˆ‘áµ¢â‚Œâ‚â¿ i = n(n+1)/2

4. **è¡¨æƒ…ç¬¦å·æµ‹è¯• Emoji Test**
   ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼â˜€ï¸ğŸŒ¤ï¸â›…ğŸŒ¦ï¸ğŸŒ§ï¸â›ˆï¸ğŸŒ©ï¸ğŸŒ¨ï¸â„ï¸â˜ƒï¸â›„ğŸŒ¬ï¸ğŸ’¨ğŸŒªï¸ğŸŒ«ï¸ğŸŒŠğŸ’§ğŸ’¦â˜”âš¡ğŸ”¥ğŸ’¥â„ï¸

5. **ç‰¹æ®Šæ ¼å¼ Special Formatting**
   **ç²—ä½“ Bold Ğ¶Ğ¸Ñ€Ğ½Ñ‹Ğ¹ Ø¹Ø±ÙŠØ¶** *æ–œä½“ Italic ĞºÑƒÑ€ÑĞ¸Ğ² Ù…Ø§Ø¦Ù„* ~~åˆ é™¤çº¿ Strikethrough Ğ·Ğ°Ñ‡ĞµÑ€ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ ÙŠØªÙˆØ³Ø·Ù‡ Ø®Ø·~~
   `ä»£ç  Code ĞºĞ¾Ğ´ ÙƒÙˆØ¯` [é“¾æ¥ Link ÑÑÑ‹Ğ»ĞºĞ° Ø±Ø§Ø¨Ø·](https://example.com)

### æ€§èƒ½æµ‹è¯•æ•°æ® Performance Test Data Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡

| è¯­è¨€ Language | å­—ç¬¦æ•° Chars | é¢„æœŸä»¤ç‰Œ Expected Tokens | å®é™…ä»¤ç‰Œ Actual Tokens | å‡†ç¡®ç‡ Accuracy |
|---------------|--------------|--------------------------|------------------------|-----------------|
| ä¸­æ–‡ Chinese | 1000 | 500 | 485 | 97.0% |
| English | 1000 | 250 | 248 | 99.2% |
| Ğ ÑƒÑÑĞºĞ¸Ğ¹ | 1000 | 200 | 195 | 97.5% |
| Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© | 1000 | 180 | 175 | 97.2% |
| æ—¥æœ¬èª | 1000 | 400 | 390 | 97.5% |
| í•œêµ­ì–´ | 1000 | 300 | 295 | 98.3% |
| Mixed æ··åˆ | 1000 | 280 | 275 | 98.2% |

è¿™ä¸ªæµ‹è¯•æ–‡æ¡£åŒ…å«äº†å„ç§è¯­è¨€ã€å­—ç¬¦é›†ã€ç¬¦å·å’Œæ ¼å¼ï¼Œç”¨äºå…¨é¢æµ‹è¯•ä»¤ç‰ŒåŒ–ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œæ€§èƒ½ã€‚
This test document contains various languages, character sets, symbols, and formats for comprehensive testing of tokenization system accuracy and performance.
Ğ­Ñ‚Ğ¾Ñ‚ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.
ØªØ­ØªÙˆÙŠ Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±ÙŠØ© Ø¹Ù„Ù‰ Ù„ØºØ§Øª ÙˆØ£Ø­Ø±Ù ÙˆØ±Ù…ÙˆØ² ÙˆØªÙ†Ø³ÙŠÙ‚Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ø¯Ù‚Ø© ÙˆØ£Ø¯Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ±Ù…ÙŠØ².
"#.to_string()
}

/// Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
fn generate_very_large_text() -> String {
    let base_text = generate_technical_doc();
    let mut large_text = String::new();
    
    // ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ 10 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
    for i in 0..10 {
        large_text.push_str(&format!("\n\n=== SECTION {} ===\n\n", i + 1));
        large_text.push_str(&base_text);
    }
    
    large_text
}

/// ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¾Ñ‚ Google API
async fn get_google_token_count(
    client: &Client, 
    api_key: &str, 
    text: &str
) -> Result<usize, Box<dyn Error + Send + Sync>> {
    let url = format!(
        "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:countTokens?key={}", 
        api_key
    );
    
    let request_body = json!({
        "contents": [{
            "parts": [{
                "text": text
            }]
        }]
    });
    
    let response = client
        .post(&url)
        .header("Content-Type", "application/json")
        .json(&request_body)
        .timeout(Duration::from_secs(30)) // Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²
        .send()
        .await?;
    
    if !response.status().is_success() {
        return Err(format!("Google API error: {}", response.status()).into());
    }
    
    let response_json: Value = response.json().await?;
    
    let total_tokens = response_json
        .get("totalTokens")
        .and_then(|t| t.as_u64())
        .ok_or("Missing totalTokens in response")?;
    
    Ok(total_tokens as usize)
}

/// Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ°Ñ…
fn calculate_accuracy(our_count: usize, google_count: usize) -> f64 {
    if google_count == 0 {
        return if our_count == 0 { 100.0 } else { 0.0 };
    }
    
    let diff = (our_count as i32 - google_count as i32).abs() as f64;
    let accuracy = (1.0 - (diff / google_count as f64)) * 100.0;
    accuracy.max(0.0)
}