// tests/google_api_token_comparison.rs

use std::env;
use std::error::Error;
use serde_json::{json, Value};
use reqwest::Client;
use tokio::time::{sleep, Duration};
use tracing::{warn, debug, error};
use gemini_proxy::tokenizer;

/// –¢–µ—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω–∞—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å —Ä–µ–∞–ª—å–Ω—ã–º API Google Gemini
/// –≠—Ç–æ—Ç —Ç–µ—Å—Ç –ø–æ–º–æ–∂–µ—Ç –≤—ã—è–≤–∏—Ç—å —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –∏ –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞—Ç—å –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä

#[tokio::test]
async fn test_token_count_accuracy_vs_google_api() {
    // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    tracing_subscriber::fmt::init();
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞
    let api_key = match env::var("GOOGLE_API_KEY") {
        Ok(key) => key,
        Err(_) => {
            println!("GOOGLE_API_KEY not found, skipping Google API comparison test");
            return;
        }
    };
    
    // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    if let Err(e) = tokenizer::gemini_simple::GeminiTokenizer::initialize().await {
        panic!("Failed to initialize our tokenizer: {e}");
    }
    
    // –¢–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏ —Ä–∞–∑–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    let test_cases = vec![
        // –ü—Ä–æ—Å—Ç—ã–µ —Å–ª—É—á–∞–∏
        "Hello",
        "Hello world",
        "Hello, world!",
        
        // –°—Ä–µ–¥–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        "The quick brown fox jumps over the lazy dog.",
        "What is the capital of France?",
        "Explain quantum computing in simple terms.",
        
        // –°–ª–æ–∂–Ω—ã–µ —Å–ª—É—á–∞–∏
        "Write a Python function to calculate fibonacci numbers recursively and iteratively.",
        "Translate the following text from English to Spanish: 'Hello, how are you today? I hope you're having a wonderful day!'",
        "Create a detailed explanation of how machine learning algorithms work, including supervised and unsupervised learning approaches.",
        
        // –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.",
        
        // –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ Unicode
        "Hello ‰∏ñÁïå! üåç How are you? –ü—Ä–∏–≤–µ—Ç –º–∏—Ä! ¬øC√≥mo est√°s?",
        "Mathematical symbols: ‚àë, ‚à´, ‚àÇ, ‚àá, ‚àû, œÄ, Œ±, Œ≤, Œ≥, Œ¥",
        
        // –ö–æ–¥
        r#"
        function fibonacci(n) {
            if (n <= 1) return n;
            return fibonacci(n - 1) + fibonacci(n - 2);
        }
        console.log(fibonacci(10));
        "#,
        
        // JSON
        r#"{"name": "John", "age": 30, "city": "New York", "hobbies": ["reading", "swimming", "coding"]}"#,
    ];
    
    let client = Client::new();
    let mut total_tests = 0;
    let mut accurate_tests = 0;
    let mut total_our_tokens = 0;
    let mut total_google_tokens = 0;
    
    println!("\n=== Token Count Comparison: Our Tokenizer vs Google API ===\n");
    
    for (i, text) in test_cases.iter().enumerate() {
        println!("Test case {}: \"{}\"", i + 1, 
            if text.chars().count() > 50 { 
                format!("{}...", text.chars().take(50).collect::<String>()) 
            } else { 
                text.to_string() 
            }
        );
        
        // –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç –Ω–∞—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        let our_count = match tokenizer::count_gemini_tokens(text) {
            Ok(count) => count,
            Err(e) => {
                error!("Our tokenizer failed for text: {}", e);
                continue;
            }
        };
        
        // –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç Google API
        let google_count = match get_google_token_count(&client, &api_key, text).await {
            Ok(count) => count,
            Err(e) => {
                warn!("Google API failed for text: {}", e);
                // –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
                sleep(Duration::from_millis(1000)).await;
                continue;
            }
        };
        
        total_tests += 1;
        total_our_tokens += our_count;
        total_google_tokens += google_count;
        
        // –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
        let accuracy = if google_count > 0 {
            let diff = (our_count as i32 - google_count as i32).abs() as f64;
            let accuracy = (1.0 - (diff / google_count as f64)) * 100.0;
            accuracy.max(0.0)
        } else if our_count == 0 { 100.0 } else { 0.0 };
        
        // –°—á–∏—Ç–∞–µ–º —Ç–µ—Å—Ç —Ç–æ—á–Ω—ã–º –µ—Å–ª–∏ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–µ–Ω–µ–µ 10%
        if accuracy >= 90.0 {
            accurate_tests += 1;
        }
        
        println!("  Our count: {our_count} | Google count: {google_count} | Accuracy: {accuracy:.1}%");
        
        if accuracy < 90.0 {
            println!("  ‚ö†Ô∏è  Significant discrepancy detected!");
        }
        
        // –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ API
        sleep(Duration::from_millis(500)).await;
    }
    
    // –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    println!("\n=== Summary ===");
    println!("Total tests: {total_tests}");
    println!("Accurate tests (>90%): {accurate_tests}");
    println!("Overall accuracy: {:.1}%", (accurate_tests as f64 / total_tests as f64) * 100.0);
    
    let overall_ratio = if total_google_tokens > 0 {
        total_our_tokens as f64 / total_google_tokens as f64
    } else {
        1.0
    };
    
    println!("Total our tokens: {total_our_tokens}");
    println!("Total Google tokens: {total_google_tokens}");
    println!("Overall ratio (our/google): {overall_ratio:.3}");
    
    if overall_ratio > 2.0 {
        println!("üö® CRITICAL: Our tokenizer counts more than 2x Google's count!");
        println!("This explains the discrepancy you mentioned.");
    } else if overall_ratio > 1.5 {
        println!("‚ö†Ô∏è  WARNING: Our tokenizer counts significantly more than Google");
    } else if overall_ratio < 0.5 {
        println!("‚ö†Ô∏è  WARNING: Our tokenizer counts significantly less than Google");
    } else {
        println!("‚úÖ Token counts are reasonably close");
    }
    
    // –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
    println!("\n=== Recommendations ===");
    if overall_ratio > 1.2 {
        println!("- Consider reducing token estimates in our tokenizer");
        println!("- Review word splitting logic");
        println!("- Check punctuation handling");
    } else if overall_ratio < 0.8 {
        println!("- Consider increasing token estimates in our tokenizer");
        println!("- Review subword tokenization");
    }
    
    // –¢–µ—Å—Ç –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–π—Ç–∏ –µ—Å–ª–∏ –æ–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å > 50% (–±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –ø–æ—Ä–æ–≥)
    let overall_accuracy = (accurate_tests as f64 / total_tests as f64) * 100.0;
    assert!(overall_accuracy >= 50.0, 
        "Overall accuracy {overall_accuracy:.1}% is below 50% threshold");
}

/// –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç Google API
async fn get_google_token_count(
    client: &Client, 
    api_key: &str, 
    text: &str
) -> Result<usize, Box<dyn Error + Send + Sync>> {
    let url = format!(
        "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:countTokens?key={api_key}"
    );
    
    let request_body = json!({
        "contents": [{
            "parts": [{
                "text": text
            }]
        }]
    });
    
    debug!("Sending request to Google API: {}", url);
    debug!("Request body: {}", serde_json::to_string_pretty(&request_body)?);
    
    let response = client
        .post(&url)
        .header("Content-Type", "application/json")
        .json(&request_body)
        .send()
        .await?;
    
    let status = response.status();
    let response_text = response.text().await?;
    
    debug!("Google API response status: {}", status);
    debug!("Google API response body: {}", response_text);
    
    if !status.is_success() {
        return Err(format!("Google API error {status}: {response_text}").into());
    }
    
    let response_json: Value = serde_json::from_str(&response_text)?;
    
    let total_tokens = response_json
        .get("totalTokens")
        .and_then(|t| t.as_u64())
        .ok_or("Missing totalTokens in response")?;
    
    Ok(total_tokens as usize)
}

#[tokio::test]
async fn test_multimodal_token_accuracy() {
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞
    let api_key = match env::var("GOOGLE_API_KEY") {
        Ok(key) => key,
        Err(_) => {
            println!("GOOGLE_API_KEY not found, skipping multimodal comparison test");
            return;
        }
    };
    
    // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã
    if let Err(e) = tokenizer::gemini_simple::GeminiTokenizer::initialize().await {
        panic!("Failed to initialize our tokenizer: {e}");
    }
    
    tokenizer::multimodal::MultimodalTokenizer::initialize(None)
        .expect("Failed to initialize multimodal tokenizer");
    
    // –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (1x1 PNG)
    let tiny_png_base64 = "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg==";
    
    let test_cases = [
        // –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
        json!({
            "contents": [{
                "parts": [{
                    "text": "What do you see in this image?"
                }]
            }]
        }),
        
        // –¢–µ–∫—Å—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        json!({
            "contents": [{
                "parts": [
                    {
                        "text": "Describe this image:"
                    },
                    {
                        "inline_data": {
                            "mime_type": "image/png",
                            "data": tiny_png_base64
                        }
                    }
                ]
            }]
        }),
    ];
    
    let client = Client::new();
    
    println!("\n=== Multimodal Token Count Comparison ===\n");
    
    for (i, test_case) in test_cases.iter().enumerate() {
        println!("Multimodal test case {}", i + 1);
        
        // –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç Google API
        let google_count = match get_google_multimodal_token_count(&client, &api_key, test_case).await {
            Ok(count) => count,
            Err(e) => {
                warn!("Google API failed for multimodal content: {}", e);
                sleep(Duration::from_millis(1000)).await;
                continue;
            }
        };
        
        println!("  Google count: {google_count}");
        
        // –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        sleep(Duration::from_millis(1000)).await;
    }
}

/// –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è multimodal –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ—Ç Google API
async fn get_google_multimodal_token_count(
    client: &Client,
    api_key: &str,
    contents: &Value
) -> Result<usize, Box<dyn Error + Send + Sync>> {
    let url = format!(
        "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:countTokens?key={api_key}"
    );
    
    debug!("Sending multimodal request to Google API: {}", url);
    debug!("Request body: {}", serde_json::to_string_pretty(contents)?);
    
    let response = client
        .post(&url)
        .header("Content-Type", "application/json")
        .json(contents)
        .send()
        .await?;
    
    let status = response.status();
    let response_text = response.text().await?;
    
    debug!("Google API multimodal response status: {}", status);
    debug!("Google API multimodal response body: {}", response_text);
    
    if !status.is_success() {
        return Err(format!("Google API error {status}: {response_text}").into());
    }
    
    let response_json: Value = serde_json::from_str(&response_text)?;
    
    let total_tokens = response_json
        .get("totalTokens")
        .and_then(|t| t.as_u64())
        .ok_or("Missing totalTokens in response")?;
    
    Ok(total_tokens as usize)
}

/// –¢–µ—Å—Ç –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –Ω–∞—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö Google API
#[tokio::test]
async fn test_calibrate_tokenizer() {
    let api_key = match env::var("GOOGLE_API_KEY") {
        Ok(key) => key,
        Err(_) => {
            println!("GOOGLE_API_KEY not found, skipping calibration test");
            return;
        }
    };
    
    // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    if let Err(e) = tokenizer::gemini_simple::GeminiTokenizer::initialize().await {
        panic!("Failed to initialize our tokenizer: {e}");
    }
    
    // –ö–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
    let calibration_texts = vec![
        "Hello",
        "Hello world",
        "The quick brown fox",
        "The quick brown fox jumps over the lazy dog",
        "This is a longer sentence with more complex vocabulary and punctuation.",
        "Write a Python function that calculates the factorial of a number using recursion.",
    ];
    
    let client = Client::new();
    let mut calibration_data = Vec::new();
    
    println!("\n=== Tokenizer Calibration Data ===\n");
    
    for text in calibration_texts {
        let our_count = tokenizer::count_gemini_tokens(text).unwrap();
        
        match get_google_token_count(&client, &api_key, text).await {
            Ok(google_count) => {
                let ratio = our_count as f64 / google_count as f64;
                calibration_data.push((text.len(), our_count, google_count, ratio));
                
                println!("Text length: {} chars", text.len());
                println!("  Our: {our_count} | Google: {google_count} | Ratio: {ratio:.3}");
                println!("  Text: \"{}\"", if text.len() > 50 { format!("{}...", &text[..50]) } else { text.to_string() });
                println!();
            }
            Err(e) => {
                warn!("Failed to get Google count for calibration: {}", e);
            }
        }
        
        sleep(Duration::from_millis(500)).await;
    }
    
    // –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    if !calibration_data.is_empty() {
        let avg_ratio: f64 = calibration_data.iter().map(|(_, _, _, ratio)| ratio).sum::<f64>() / calibration_data.len() as f64;
        
        println!("=== Calibration Analysis ===");
        println!("Average ratio (our/google): {avg_ratio:.3}");
        
        if avg_ratio > 1.5 {
            println!("üö® RECOMMENDATION: Reduce token estimates by factor of {:.2}", 1.0 / avg_ratio);
        } else if avg_ratio < 0.7 {
            println!("üö® RECOMMENDATION: Increase token estimates by factor of {:.2}", 1.0 / avg_ratio);
        } else {
            println!("‚úÖ Token estimates are reasonably calibrated");
        }
        
        // –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ—á–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
        let correction_factor = 1.0 / avg_ratio;
        println!("Suggested correction factor: {correction_factor:.3}");
        println!("Apply this factor to your tokenizer estimates to improve accuracy.");
    }
}