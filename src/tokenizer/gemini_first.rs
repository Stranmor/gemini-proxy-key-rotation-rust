// src/tokenizer/gemini_first.rs

use std::error::Error;
use std::sync::OnceLock;
use tracing::{info, warn, debug};

/// "Gemini First" —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä - –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –Ω–∞–ø—Ä—è–º—É—é –∫ Gemini
/// –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (–ª–∏–º–∏—Ç—ã, –±–∏–ª–ª–∏–Ω–≥)
pub struct GeminiFirstTokenizer {
    config: GeminiFirstConfig,
}

#[derive(Debug, Clone)]
pub struct GeminiFirstConfig {
    /// –í–∫–ª—é—á–∏—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Ç–æ–∫–µ–Ω–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é false)
    pub enable_pre_check: bool,
    /// –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
    pub pre_check_limit: Option<usize>,
    /// –í–∫–ª—é—á–∏—Ç—å –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ (–¥–ª—è –±–∏–ª–ª–∏–Ω–≥–∞)
    pub enable_post_count: bool,
    /// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—ã—Å—Ç—Ä—É—é –æ—Ü–µ–Ω–∫—É –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    pub use_fast_estimation: bool,
    /// –ü–æ—Ä–æ–≥ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏ (—Å–∏–º–≤–æ–ª—ã)
    pub fast_estimation_threshold: usize,
}

impl Default for GeminiFirstConfig {
    fn default() -> Self {
        Self {
            enable_pre_check: false,        // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ä–∞–∑—É
            pre_check_limit: None,          // –ë–µ–∑ –ª–∏–º–∏—Ç–æ–≤
            enable_post_count: true,        // –°—á–∏—Ç–∞–µ–º –ø–æ—Å–ª–µ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            use_fast_estimation: true,      // –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
            fast_estimation_threshold: 50000, // 50KB –ø–æ—Ä–æ–≥
        }
    }
}

static GEMINI_FIRST_TOKENIZER: OnceLock<GeminiFirstTokenizer> = OnceLock::new();

impl GeminiFirstTokenizer {
    /// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç "Gemini First" —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    pub fn initialize(config: Option<GeminiFirstConfig>) -> Result<(), Box<dyn Error + Send + Sync>> {
        let config = config.unwrap_or_default();
        
        info!("Initializing Gemini First tokenizer (direct API approach)");
        info!("Pre-check enabled: {}", config.enable_pre_check);
        info!("Fast estimation threshold: {} chars", config.fast_estimation_threshold);
        
        let tokenizer = Self { config };
        
        match GEMINI_FIRST_TOKENIZER.set(tokenizer) {
            Ok(_) => info!("Gemini First tokenizer initialized successfully"),
            Err(_) => warn!("Gemini First tokenizer was already initialized"),
        }
        
        Ok(())
    }
    
    /// –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥: —Ä–µ—à–∞–µ—Ç –Ω—É–∂–Ω–∞ –ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ä–∞–∑—É
    pub fn should_tokenize_before_request(&self, text: &str) -> TokenizationDecision {
        let text_length = text.len();
        
        // –ï—Å–ª–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ä–∞–∑—É
        if !self.config.enable_pre_check {
            debug!("Pre-check disabled, sending directly to Gemini");
            return TokenizationDecision::SendDirectly;
        }
        
        // –ï—Å–ª–∏ –µ—Å—Ç—å –ª–∏–º–∏—Ç –∏ —Ç–µ–∫—Å—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π - –±—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞
        if let Some(limit) = self.config.pre_check_limit {
            if text_length > self.config.fast_estimation_threshold {
                let estimated_tokens = self.fast_estimate_tokens(text);
                
                if estimated_tokens > limit {
                    debug!("Fast estimation: {} tokens > {} limit, rejecting", estimated_tokens, limit);
                    return TokenizationDecision::RejectTooLarge(estimated_tokens);
                } else {
                    debug!("Fast estimation: {} tokens < {} limit, sending directly", estimated_tokens, limit);
                    return TokenizationDecision::SendDirectly;
                }
            }
        }
        
        // –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å —Ç–æ—á–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if text_length < 10000 && self.config.pre_check_limit.is_some() {
            return TokenizationDecision::TokenizeFirst;
        }
        
        // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ä–∞–∑—É
        TokenizationDecision::SendDirectly
    }
    
    /// –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ (–¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤)
    fn fast_estimate_tokens(&self, text: &str) -> usize {
        let char_count = text.chars().count();
        
        // –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Å—Ç–æ–≤:
        // - –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç: ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        // - –ö–æ–¥: ~3.5 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω  
        // - Unicode: ~2.5 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        // - JSON: ~3.2 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        
        let unicode_ratio = text.chars().filter(|c| !c.is_ascii()).count() as f64 / char_count as f64;
        let code_indicators = text.matches('{').count() + text.matches('}').count() + 
                             text.matches(';').count() + text.matches("function").count();
        let json_indicators = text.matches('"').count() + text.matches(':').count();
        
        let chars_per_token = if unicode_ratio > 0.2 {
            2.5 // –ú–Ω–æ–≥–æ Unicode
        } else if code_indicators > char_count / 100 {
            3.5 // –ú–Ω–æ–≥–æ –∫–æ–¥–∞
        } else if json_indicators > char_count / 50 {
            3.2 // JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        } else {
            4.0 // –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
        };
        
        let estimated = (char_count as f64 / chars_per_token).ceil() as usize;
        
        debug!("Fast estimation: {} chars ‚Üí {} tokens (ratio: {:.1})", 
            char_count, estimated, chars_per_token);
        
        estimated
    }
    
    /// –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ (–¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏)
    pub fn count_tokens_post_response(&self, request_text: &str, response_text: &str) -> PostResponseTokens {
        if !self.config.enable_post_count {
            return PostResponseTokens::default();
        }
        
        // –î–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—É—é –æ—Ü–µ–Ω–∫—É
        let request_tokens = if request_text.len() > self.config.fast_estimation_threshold {
            self.fast_estimate_tokens(request_text)
        } else {
            // –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç
            self.accurate_count_small_text(request_text)
        };
        
        let response_tokens = if response_text.len() > 5000 {
            self.fast_estimate_tokens(response_text)
        } else {
            self.accurate_count_small_text(response_text)
        };
        
        PostResponseTokens {
            request_tokens,
            response_tokens,
            total_tokens: request_tokens + response_tokens,
            estimation_used: request_text.len() > self.config.fast_estimation_threshold,
        }
    }
    
    /// –¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    fn accurate_count_small_text(&self, text: &str) -> usize {
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
        let words = text.split_whitespace().count();
        let punctuation = text.chars().filter(|c| c.is_ascii_punctuation()).count();
        let unicode_chars = text.chars().filter(|c| !c.is_ascii()).count();
        
        // –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        let base_tokens = words + punctuation / 2;
        
        // –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª—è Unicode
        let unicode_adjustment = if unicode_chars > 0 {
            (unicode_chars as f64 * 0.7) as usize
        } else {
            0
        };
        
        (base_tokens + unicode_adjustment).max(1)
    }
    
    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    pub fn get_config(&self) -> &GeminiFirstConfig {
        &self.config
    }
    
    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ
    pub fn get_info(&self) -> String {
        format!(
            "Gemini First Tokenizer (pre-check: {}, fast estimation: {})",
            self.config.enable_pre_check,
            self.config.use_fast_estimation
        )
    }
}

/// –†–µ—à–µ–Ω–∏–µ –æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
#[derive(Debug, Clone)]
pub enum TokenizationDecision {
    /// –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞–ø—Ä—è–º—É—é –∫ Gemini (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
    SendDirectly,
    /// –°–Ω–∞—á–∞–ª–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å (—Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤)
    TokenizeFirst,
    /// –û—Ç–∫–ª–æ–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å - —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤
    RejectTooLarge(usize),
}

/// –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞
#[derive(Debug, Clone, Default)]
pub struct PostResponseTokens {
    pub request_tokens: usize,
    pub response_tokens: usize,
    pub total_tokens: usize,
    pub estimation_used: bool,
}

/// –ü–æ–ª—É—á–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
pub fn get_gemini_first_tokenizer() -> Option<&'static GeminiFirstTokenizer> {
    GEMINI_FIRST_TOKENIZER.get()
}

/// –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω—É–∂–Ω–∞ –ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
pub fn should_tokenize_before_request(text: &str) -> TokenizationDecision {
    match GEMINI_FIRST_TOKENIZER.get() {
        Some(tokenizer) => tokenizer.should_tokenize_before_request(text),
        None => {
            warn!("Gemini First tokenizer not initialized, sending directly");
            TokenizationDecision::SendDirectly
        }
    }
}

/// –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞
pub fn count_tokens_post_response(request_text: &str, response_text: &str) -> PostResponseTokens {
    match GEMINI_FIRST_TOKENIZER.get() {
        Some(tokenizer) => tokenizer.count_tokens_post_response(request_text, response_text),
        None => PostResponseTokens::default(),
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_gemini_first_initialization() {
        // –¢–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ - –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–µ –ø–∞–¥–∞–µ—Ç
        let result = GeminiFirstTokenizer::initialize(None);
        assert!(result.is_ok());
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω
        let tokenizer = get_gemini_first_tokenizer();
        assert!(tokenizer.is_some());
    }
    
    #[test]
    fn test_should_send_directly_by_default() {
        GeminiFirstTokenizer::initialize(None).unwrap();
        
        let decision = should_tokenize_before_request("This is a test message");
        matches!(decision, TokenizationDecision::SendDirectly);
    }
    
    #[test]
    fn test_fast_estimation() {
        let config = GeminiFirstConfig {
            enable_pre_check: true,
            pre_check_limit: Some(1000),
            use_fast_estimation: true,
            fast_estimation_threshold: 100,
            enable_post_count: true,
        };
        
        GeminiFirstTokenizer::initialize(Some(config)).unwrap();
        
        // –ë–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç - –¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—ã—Å—Ç—Ä—É—é –æ—Ü–µ–Ω–∫—É
        let large_text = "Hello world! ".repeat(1000); // ~13000 —Å–∏–º–≤–æ–ª–æ–≤
        let decision = should_tokenize_before_request(&large_text);
        
        // –î–æ–ª–∂–µ–Ω –æ—Ç–∫–ª–æ–Ω–∏—Ç—å –∫–∞–∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
        matches!(decision, TokenizationDecision::RejectTooLarge(_));
    }
    
    #[test]
    fn test_post_response_counting() {
        GeminiFirstTokenizer::initialize(None).unwrap();
        
        let request = "What is the capital of France?";
        let response = "The capital of France is Paris.";
        
        let tokens = count_tokens_post_response(request, response);
        
        assert!(tokens.request_tokens > 0);
        assert!(tokens.response_tokens > 0);
        assert_eq!(tokens.total_tokens, tokens.request_tokens + tokens.response_tokens);
        assert!(!tokens.estimation_used); // –ù–µ–±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç
    }
    
    #[test]
    fn test_unicode_handling() {
        GeminiFirstTokenizer::initialize(None).unwrap();
        
        let unicode_text = "Hello ‰∏ñÁïå! üåç How are you? –ü—Ä–∏–≤–µ—Ç –º–∏—Ä!";
        let tokens = count_tokens_post_response(unicode_text, "");
        
        assert!(tokens.request_tokens > 0);
        println!("Unicode text tokens: {}", tokens.request_tokens);
    }
    
    #[test]
    fn test_performance_on_large_text() {
        GeminiFirstTokenizer::initialize(None).unwrap();
        
        // –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç ~180k —Ç–æ–∫–µ–Ω–æ–≤ (–∫–∞–∫ –≤ –≤–∞—à–µ–º —Å–ª—É—á–∞–µ)
        let large_text = "This is a comprehensive test document with various content types. ".repeat(10000);
        println!("Large text size: {} chars", large_text.len());
        
        let start = std::time::Instant::now();
        let decision = should_tokenize_before_request(&large_text);
        let decision_time = start.elapsed();
        
        let start = std::time::Instant::now();
        let tokens = count_tokens_post_response(&large_text, "Response");
        let counting_time = start.elapsed();
        
        println!("Decision time: {:?}", decision_time);
        println!("Counting time: {:?}", counting_time);
        println!("Estimated tokens: {}", tokens.request_tokens);
        
        // –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ
        assert!(decision_time.as_millis() < 5);
        assert!(counting_time.as_millis() < 10);
        
        // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–æ–ª–∂–µ–Ω –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —Å—Ä–∞–∑—É
        matches!(decision, TokenizationDecision::SendDirectly);
    }
}